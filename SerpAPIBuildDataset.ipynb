{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa13f8f3-bffc-468d-aee2-a209fc47cfd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get list of papers citing each top n most sited paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02a7b1",
   "metadata": {},
   "source": [
    "AI research papers were previously collected to a file called \"WoS_All_Most_cited.xls\".\n",
    "This notebook contains the code used to fech and connect citation info to papers in this data set.\n",
    "This is done to N-most cited papers from the data set, because of the limited nuber of searches available.\n",
    "SerpAPI is used to get this infromation from Google Scholar.\n",
    "Pairs of papers are then created and written into files for use in a machine learning task.\n",
    "One file contains pairs of papers between which exist a citation relationship, the other contains\n",
    "randomly sampled pairs without a citation betweent them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e5b8eb-40c9-4e01-ac71-12fedd2b3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1ce283-ea94-4dac-9587-ec753c299863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458a0c10-e9f9-4587-a72c-fef00bf2421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Publication Type', 'Authors', 'Book Authors',\n",
      "       'Book Editors', 'Book Group Authors', 'Author Full Names',\n",
      "       'Book Author Full Names', 'Group Authors', 'Article Title',\n",
      "       'Source Title', 'Book Series Title', 'Book Series Subtitle', 'Language',\n",
      "       'Document Type', 'Conference Title', 'Conference Date',\n",
      "       'Conference Location', 'Conference Sponsor', 'Conference Host',\n",
      "       'Author Keywords', 'Keywords Plus', 'Abstract', 'Addresses',\n",
      "       'Affiliations', 'Reprint Addresses', 'Email Addresses',\n",
      "       'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred',\n",
      "       'Funding Text', 'Cited References', 'Cited Reference Count',\n",
      "       'Times Cited, WoS Core', 'Times Cited, All Databases',\n",
      "       '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher',\n",
      "       'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
      "       'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
      "       'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement',\n",
      "       'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page',\n",
      "       'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date',\n",
      "       'Number of Pages', 'WoS Categories', 'Web of Science Index',\n",
      "       'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations',\n",
      "       'Highly Cited Status', 'Hot Paper Status', 'Date of Export',\n",
      "       'UT (Unique WOS ID)', 'Web of Science Record'],\n",
      "      dtype='object')\n",
      "(8840, 73)\n"
     ]
    }
   ],
   "source": [
    "excel_filename = \"WoS_All_Most_cited\"\n",
    "df = pd.read_excel(excel_filename + \".xls\")\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38fa8f-ab85-4530-98f0-7620e806539d",
   "metadata": {},
   "source": [
    "### DOI is used for the search - Drop rows with NaN DOI and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4eb0a1-3c80-47b0-9881-3edf54caa29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "nan_count = df['DOI'].isna().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f71d7e0-f54f-48cc-b8cd-ce75a1e4eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "df = df[df['DOI'].notna()]\n",
    "nan_count = df['DOI'].isna().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2339a8a3-3182-4f1d-bb6b-1abf8f4fbfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8793, 73)\n",
      "(8791, 73)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.drop_duplicates(subset=[\"DOI\"], inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc08d20-e537-481d-bab2-d37be649c034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1109/TITS.2020.3024655'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ed65a5-731a-4788-b21f-4eb9c20902e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "nan_count = df['Abstract'].isna().sum()\n",
    "print(nan_count)\n",
    "df = df[df['Abstract'].notna()]\n",
    "nan_count = df['Abstract'].isna().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e74539-3edc-40e0-aa26-8d008cc6abe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].loc[\"Article Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df618ef-45b6-4456-92bb-9c3685778923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('Times Cited, All Databases', axis=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5246e74b-71a4-48a5-8eca-29329f609595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566     21220\n",
       "4897    15358\n",
       "270     10433\n",
       "32       8598\n",
       "3224     8036\n",
       "Name: Times Cited, All Databases, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()['Times Cited, All Databases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c60496cb-2ab7-4648-89e0-3fe912895a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konna\\AppData\\Local\\Temp\\ipykernel_17988\\3962508928.py:1: FutureWarning: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas. This is the only engine in pandas that supports writing in the xls format. Install openpyxl and write to an xlsx file instead. You can set the option io.excel.xls.writer to 'xlwt' to silence this warning. While this option is deprecated and will also raise a warning, it can be globally set and the warning suppressed.\n",
      "  df.to_excel(\"WoS_All_Most_cited_nCitations_decending.xls\")\n"
     ]
    }
   ],
   "source": [
    "df.to_excel(\"WoS_All_Most_cited_nCitations_decending.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cac15432-ce48-4844-8032-6bb6e50fe16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in c:\\users\\konna\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\konna\\anaconda3\\lib\\site-packages (from google-search-results) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\konna\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\konna\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\konna\\anaconda3\\lib\\site-packages (from requests->google-search-results) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\konna\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.3)\n"
     ]
    }
   ],
   "source": [
    "# Install SerpApi\n",
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f94e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2531\n",
      "10.1038/nature14539\n"
     ]
    }
   ],
   "source": [
    "# For reattempting failed searches\n",
    "# If searcheas have already been done, and the files created during this exist, remove the quotations to activate this cell\n",
    "# to not repead searches that have already been successful\n",
    "already_searched_dois = []\n",
    "\"\"\"\n",
    "with open (\"SearchIDs.txt\", 'r') as s_id_file:\n",
    "    while True:\n",
    "        s_id = s_id_file.readline().rstrip(\"\\n\")\n",
    "        if s_id != \"\":\n",
    "            already_searched_dois.append(s_id.split(\" \")[1])\n",
    "        else:\n",
    "            break\n",
    "print(len(already_searched_dois))\n",
    "print(already_searched_dois[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a4b8c7-2394-4788-b0e7-4baf999aaa24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\n#Add top cited papers\\nn_searches = 2550\\nimport json\\nfrom serpapi import GoogleSearch\\njson_obj_list_search = []\\nwith open(\"TopCited2.json\", \\'w\\') as json_file:\\n    with open (\"SearchIDs2.txt\", \\'w\\') as s_id_file:\\n        json_file.write(\"{\\n\")\\n        n_discarded = 0\\n        for i in range(n_searches):\\n            doi = df.iloc[i][\\'DOI\\']\\n            params = {\\n                \"engine\": \"google_scholar\",\\n                \"q\": doi,\\n                \"api_key\": \"9e76cb77baffe66b33207f987b436521cf9714b849e7c8b4abd59c4ff99c5308\"\\n            }\\n            s_id = str(\"search_\" + str(i) + \": \" + doi)\\n            if doi not in already_searched_dois:\\n                try:\\n                    result = GoogleSearch(params).get_dict()\\n                    if type(result) is dict and result.get(\"organic_results\") != None:\\n                        organic_result = result[\"organic_results\"]\\n                        if len(organic_result) > 0 and result.get(\"search_metadata\").get(\"status\") == \"Success\" and organic_result[0].get(\"inline_links\").get(\"cited_by\") != None:\\n                            json_file.write(str(\\'\"\\' + s_id + \\'\": \\'))\\n                            s_id_file.write(s_id + \"\\n\")\\n                            json.dump(organic_result, json_file, indent=4)\\n                            json_file.write(\",\\n\")\\n                    else:\\n                        n_discarded += 1\\n                except:\\n                    n_discarded += 1\\n        # Get rid of ,\\n after last entry\\n        json_file.seek(json_file.tell() - 3, os.SEEK_SET)\\n        json_file.truncate()\\n        json_file.write(\"\\n}\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#Add top cited papers\n",
    "n_searches = 2550\n",
    "import json\n",
    "from serpapi import GoogleSearch\n",
    "json_obj_list_search = []\n",
    "with open(\"TopCited2.json\", 'w') as json_file:\n",
    "    with open (\"SearchIDs2.txt\", 'w') as s_id_file:\n",
    "        json_file.write(\"{\\n\")\n",
    "        n_discarded = 0\n",
    "        for i in range(n_searches):\n",
    "            doi = df.iloc[i]['DOI']\n",
    "            params = {\n",
    "                \"engine\": \"google_scholar\",\n",
    "                \"q\": doi,\n",
    "                \"api_key\": \"9e76cb77baffe66b33207f987b436521cf9714b849e7c8b4abd59c4ff99c5308\"\n",
    "            }\n",
    "            s_id = str(\"search_\" + str(i) + \": \" + doi)\n",
    "            if doi not in already_searched_dois:\n",
    "                try:\n",
    "                    result = GoogleSearch(params).get_dict()\n",
    "                    if type(result) is dict and result.get(\"organic_results\") != None:\n",
    "                        organic_result = result[\"organic_results\"]\n",
    "                        if len(organic_result) > 0 and result.get(\"search_metadata\").get(\"status\") == \"Success\" and organic_result[0].get(\"inline_links\").get(\"cited_by\") != None:\n",
    "                            json_file.write(str('\"' + s_id + '\": '))\n",
    "                            s_id_file.write(s_id + \"\\n\")\n",
    "                            json.dump(organic_result, json_file, indent=4)\n",
    "                            json_file.write(\",\\n\")\n",
    "                    else:\n",
    "                        n_discarded += 1\n",
    "                except:\n",
    "                    n_discarded += 1\n",
    "        # Get rid of ,\\n after last entry\n",
    "        json_file.seek(json_file.tell() - 3, os.SEEK_SET)\n",
    "        json_file.truncate()\n",
    "        json_file.write(\"\\n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c36c04cb-eab1-43b1-a360-1effd538d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(n_discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caf4221f-8239-4b7b-a686-483cc8f57fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TopCited.json') as f:\n",
    "    searches = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acb699b2-fe06-4784-8503-f870f57ffb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'position': 0, 'title': 'Scaling learning algorithms towards AI', 'result_id': 'sjFkRZfnofYJ', 'type': 'Pdf', 'link': 'https://pdfs.semanticscholar.org/f01e/080777b59d6978e412ded8995edabbaa62f0.pdf', 'snippet': 'Scaling Learning Algorithms Towards AI Page 1 Scaling Learning Algorithms Towards AI Authors: Yoshua Bengio, Yann LeCun Presenter: Marilyn Vazquez George Mason University February 10, 2017 Bengion and LeCun (GMU) NLDA Seminar February 10, 2017 1 / 25 Page 2 Outline 1 Curse of Dimensionality 2 Shallow Learning 3 Deep Learning 4 Results 5 Conclusion Bengion and LeCun (GMU) NLDA Seminar February 10, 2017 2 / 25 Page 3 Curse of Dimensionality Curse of Dimensionality The curse of dimensionality can be viewed either as …', 'publication_info': {'summary': 'Y Bengio, Y LeCun - Large-scale kernel machines, 2007 - pdfs.semanticscholar.org', 'authors': [{'name': 'Y Bengio', 'link': 'https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kukA0LcAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kukA0LcAAAAJ'}, {'name': 'Y LeCun', 'link': 'https://scholar.google.com/citations?user=WLN3QrAAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=WLN3QrAAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'WLN3QrAAAAAJ'}]}, 'resources': [{'title': 'semanticscholar.org', 'file_format': 'PDF', 'link': 'https://pdfs.semanticscholar.org/f01e/080777b59d6978e412ded8995edabbaa62f0.pdf'}], 'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&q=sjFkRZfnofYJ', 'cited_by': {'total': 1717, 'link': 'https://scholar.google.com/scholar?cites=17771740241470960050&as_sdt=2005&sciodt=0,5&hl=en', 'cites_id': '17771740241470960050', 'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=2005&cites=17771740241470960050&engine=google_scholar&hl=en'}, 'related_pages_link': 'https://scholar.google.com/scholar?q=related:sjFkRZfnofYJ:scholar.google.com/&scioq=10.1038/nature14539&hl=en&as_sdt=0,5', 'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C5&engine=google_scholar&hl=en&q=related%3AsjFkRZfnofYJ%3Ascholar.google.com%2F', 'versions': {'total': 27, 'link': 'https://scholar.google.com/scholar?cluster=17771740241470960050&hl=en&as_sdt=0,5', 'cluster_id': '17771740241470960050', 'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C5&cluster=17771740241470960050&engine=google_scholar&hl=en'}, 'cached_page_link': 'https://scholar.googleusercontent.com/scholar?q=cache:sjFkRZfnofYJ:scholar.google.com/+10.1038/nature14539&hl=en&as_sdt=0,5'}}]\n"
     ]
    }
   ],
   "source": [
    "print(searches[\"search_0: 10.1038/nature14539\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52341b92-a52e-442b-a229-ddb1f68f931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566     17771740241470960050\n",
      "4897    18433510229623055695\n",
      "270     17609610056545403599\n",
      "32      18446742974550982994\n",
      "3224    16986546914786142867\n",
      "Name: Cites_id, dtype: object\n",
      "566              10.1038/nature14539\n",
      "4897                 10.1145/3065386\n",
      "270              10.1038/nature14236\n",
      "32      10.1016/j.neunet.2014.09.003\n",
      "3224      10.1109/TPAMI.2017.2699184\n",
      "Name: DOI, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame where each paper has a cites_id\n",
    "cites_ids = []\n",
    "cites_dois = []\n",
    "with open (\"SearchIDs.txt\", 'r') as s_id_file:\n",
    "    while True:\n",
    "        s_id = s_id_file.readline().rstrip(\"\\n\")\n",
    "        if s_id != \"\":\n",
    "            cites_ids.append(searches[s_id][0][\"inline_links\"][\"cited_by\"][\"cites_id\"])\n",
    "            cites_dois.append(s_id.split(\" \")[1])\n",
    "        else:\n",
    "            break\n",
    "df_top_n = df[df['DOI'].isin(cites_dois)]\n",
    "df_top_n = df_top_n.assign(Cites_id=cites_ids)\n",
    "print(df_top_n.head()[\"Cites_id\"])\n",
    "print(df_top_n.head()[\"DOI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99a21c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723\n"
     ]
    }
   ],
   "source": [
    "# For reattempting failed searches\n",
    "with open('TopCitedCitedBy.json') as f:\n",
    "    searches = json.load(f)\n",
    "    \n",
    "already_got_dois = []\n",
    "with open (\"CitedBySearchIDs.txt\", 'r') as s_id_file:\n",
    "    while True:\n",
    "        s_id = s_id_file.readline().rstrip(\"\\n\")\n",
    "        if s_id != \"\":\n",
    "            if type(searches.get(s_id)) is dict and \"error\" not in searches.get(s_id).keys():\n",
    "                already_got_dois.append(s_id.split(\" \")[1])\n",
    "        else:\n",
    "            break\n",
    "print(len(already_got_dois))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a16f4fc-2b9e-49ab-b7ca-d3002c99eaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndiscarded2 = 0\\nimport os\\nimport json\\nfrom serpapi import GoogleSearch\\n# Search for papers that cite each of the top papers\\n#Add top cited papers\\njson_obj_list_search = []\\nwith open(\"TopCitedCitedBy2.json\", \\'w\\') as json_file:\\n    with open (\"CitedBySearchIDs2.txt\", \\'w\\') as s_id_file:\\n        json_file.write(\"{\\n\")\\n        for i in range(df_top_n.shape[0]):\\n            doi = df_top_n.iloc[i][\\'DOI\\']\\n            params = {\\n                \"engine\": \"google_scholar\",\\n                \"cites\": df_top_n.iloc[i][\\'Cites_id\\'],\\n                \"api_key\": \"6449cb6519444a9134b335b095de6085f36f74b2e7ebca097d744e231012dec3\"\\n            }\\n            s_id = str(\"papers_citing_search_:\" + str(i) + \"_paper: \" + doi)\\n            if doi not in already_got_dois:\\n                try:\\n                    s_result = GoogleSearch(params)\\n                    json_file.write(str(\\'\"\\' + s_id + \\'\": \\'))\\n                    s_id_file.write(s_id + \"\\n\")\\n                    json.dump(s_result.get_dict(), json_file, indent=4)\\n                except:\\n                    discarded2 += 1\\n                if i < df_top_n.shape[0]:\\n                    json_file.write(\",\\n\")\\n        json_file.write(\"\\n}\")\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discarded2 = 0\n",
    "import os\n",
    "import json\n",
    "from serpapi import GoogleSearch\n",
    "# Search for papers that cite each of the top papers\n",
    "#Add top cited papers\n",
    "json_obj_list_search = []\n",
    "with open(\"TopCitedCitedBy2.json\", 'w') as json_file:\n",
    "    with open (\"CitedBySearchIDs2.txt\", 'w') as s_id_file:\n",
    "        json_file.write(\"{\\n\")\n",
    "        for i in range(df_top_n.shape[0]):\n",
    "            doi = df_top_n.iloc[i]['DOI']\n",
    "            params = {\n",
    "                \"engine\": \"google_scholar\",\n",
    "                \"cites\": df_top_n.iloc[i]['Cites_id'],\n",
    "                \"api_key\": \"6449cb6519444a9134b335b095de6085f36f74b2e7ebca097d744e231012dec3\"\n",
    "            }\n",
    "            s_id = str(\"papers_citing_search_:\" + str(i) + \"_paper: \" + doi)\n",
    "            if doi not in already_got_dois:\n",
    "                try:\n",
    "                    s_result = GoogleSearch(params)\n",
    "                    json_file.write(str('\"' + s_id + '\": '))\n",
    "                    s_id_file.write(s_id + \"\\n\")\n",
    "                    json.dump(s_result.get_dict(), json_file, indent=4)\n",
    "                    if i < df_top_n.shape[0]:\n",
    "                        json_file.write(\",\\n\")\n",
    "                except:\n",
    "                    discarded2 += 1\n",
    "        json_file.write(\"\\n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "316f30ab-709e-4595-848a-e79367918975",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TopCitedCitedBy.json') as f:\n",
    "    searches = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feee7c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566     17771740241470960050\n",
      "4897    18433510229623055695\n",
      "270     17609610056545403599\n",
      "32      18446742974550982994\n",
      "3224    16986546914786142867\n",
      "Name: Cites_id, dtype: object\n",
      "(2531, 74)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame where each paper has a cites_id\n",
    "cites_dois = []\n",
    "with open (\"CitedBySearchIDs.txt\", 'r') as s_id_file:\n",
    "    while True:\n",
    "        s_id = s_id_file.readline().rstrip(\"\\n\")\n",
    "        if s_id != \"\":\n",
    "            if type(searches.get(s_id)) is dict and \"error\" not in searches.get(s_id).keys():\n",
    "                cites_dois.append(s_id.split(\" \")[1])\n",
    "        else:\n",
    "            break\n",
    "df_top_n = df_top_n[df_top_n['DOI'].isin(cites_dois)]\n",
    "print(df_top_n.head()[\"Cites_id\"])\n",
    "print(df_top_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71aa29dd-9676-43e1-aa03-9b4990305352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(searches[\"papers_citing_search_:45_paper: 10.1016/j.jcp.2018.10.045\"])\n",
    "#print(searches[\"papers_citing_search_:39_paper: 10.1038/s41587-019-0036-z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0774bb4-f183-4bf1-b942-cc6c9f2ae739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Cites_id</th>\n",
       "      <th>Cited_by_cites_ids</th>\n",
       "      <th>Cited_by_titles</th>\n",
       "      <th>Cited_by_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Deep learning</td>\n",
       "      <td>17771740241470960050</td>\n",
       "      <td>[559463397382443088, 6340776138936674554, 1676...</td>\n",
       "      <td>[Representation learning: A review and new per...</td>\n",
       "      <td>[[{'name': 'Y Bengio', 'link': 'https://schola...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>ImageNet Classification with Deep Convolutiona...</td>\n",
       "      <td>18433510229623055695</td>\n",
       "      <td>[15853904744099473538, 4107479906416396359, 12...</td>\n",
       "      <td>[A comprehensive survey of image-based food re...</td>\n",
       "      <td>[[{'name': 'GA Tahir', 'link': 'https://schola...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Human-level control through deep reinforcement...</td>\n",
       "      <td>17609610056545403599</td>\n",
       "      <td>[15107175476013490880, 16771076966357002396, 1...</td>\n",
       "      <td>[Cortical excitation and chronic pain, The rol...</td>\n",
       "      <td>[[{'name': 'M Zhuo', 'link': 'https://scholar....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Deep learning in neural networks: An overview</td>\n",
       "      <td>18446742974550982994</td>\n",
       "      <td>[12864264545474230024, 2690735549741753526, 11...</td>\n",
       "      <td>[Detail review on chemical, physical and green...</td>\n",
       "      <td>[[{'name': 'A Nazir', 'link': 'https://scholar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>DeepLab: Semantic Image Segmentation with Deep...</td>\n",
       "      <td>16986546914786142867</td>\n",
       "      <td>[14852114597417537063, 16441576239143306181, 8...</td>\n",
       "      <td>[Review on remote sensing methods for landslid...</td>\n",
       "      <td>[[{'name': 'A Mohan', 'link': 'https://scholar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Article Title              Cites_id  \\\n",
       "566                                       Deep learning  17771740241470960050   \n",
       "4897  ImageNet Classification with Deep Convolutiona...  18433510229623055695   \n",
       "270   Human-level control through deep reinforcement...  17609610056545403599   \n",
       "32        Deep learning in neural networks: An overview  18446742974550982994   \n",
       "3224  DeepLab: Semantic Image Segmentation with Deep...  16986546914786142867   \n",
       "\n",
       "                                     Cited_by_cites_ids  \\\n",
       "566   [559463397382443088, 6340776138936674554, 1676...   \n",
       "4897  [15853904744099473538, 4107479906416396359, 12...   \n",
       "270   [15107175476013490880, 16771076966357002396, 1...   \n",
       "32    [12864264545474230024, 2690735549741753526, 11...   \n",
       "3224  [14852114597417537063, 16441576239143306181, 8...   \n",
       "\n",
       "                                        Cited_by_titles  \\\n",
       "566   [Representation learning: A review and new per...   \n",
       "4897  [A comprehensive survey of image-based food re...   \n",
       "270   [Cortical excitation and chronic pain, The rol...   \n",
       "32    [Detail review on chemical, physical and green...   \n",
       "3224  [Review on remote sensing methods for landslid...   \n",
       "\n",
       "                                       Cited_by_authors  \n",
       "566   [[{'name': 'Y Bengio', 'link': 'https://schola...  \n",
       "4897  [[{'name': 'GA Tahir', 'link': 'https://schola...  \n",
       "270   [[{'name': 'M Zhuo', 'link': 'https://scholar....  \n",
       "32    [[{'name': 'A Nazir', 'link': 'https://scholar...  \n",
       "3224  [[{'name': 'A Mohan', 'link': 'https://scholar...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add cited by information for each paper in top cited\n",
    "cites_ids = []\n",
    "cited_by_titles = []\n",
    "cited_by_authors = []\n",
    "paper_ids = []\n",
    "with open (\"CitedBySearchIDs.txt\", 'r') as s_id_file:\n",
    "    s_id = s_id_file.readline().rstrip(\"\\n\")\n",
    "    if type(searches.get(s_id)) is dict and \"error\" not in searches.get(s_id).keys():\n",
    "        organic_results = searches[str(s_id)][\"organic_results\"]\n",
    "        while s_id != \"\" and type(organic_results) is list:\n",
    "            temp_ids = []\n",
    "            temp_titles = []\n",
    "            temp_authors = []\n",
    "            if s_id not in paper_ids and type(searches.get(s_id)) is dict and \"error\" not in searches.get(s_id).keys():\n",
    "                paper_ids.append(s_id)\n",
    "                if len(organic_results) > 0:\n",
    "                    for result in organic_results:\n",
    "                        if type(result) is dict and type(result.get(\"inline_links\")) is dict and type(result.get(\"inline_links\").get(\"cited_by\")) is dict:\n",
    "                            temp_ids.append(result.get(\"inline_links\").get(\"cited_by\").get(\"cites_id\"))\n",
    "                        if type(result) is dict and type(result.get(\"title\")) is str:\n",
    "                            temp_titles.append(result.get(\"title\"))\n",
    "                        if type(result) is dict and type(result.get(\"publication_info\")) is dict and type(result.get(\"publication_info\").get(\"authors\")) is list:\n",
    "                            temp_authors.append(result.get(\"publication_info\").get(\"authors\"))\n",
    "                    cites_ids.append(temp_ids)\n",
    "                    cited_by_titles.append(temp_titles)\n",
    "                    cited_by_authors.append(temp_authors)\n",
    "            s_id = s_id_file.readline().rstrip(\"\\n\")\n",
    "            if type(searches.get(s_id)) is dict and \"error\" not in searches.get(s_id).keys():\n",
    "                organic_results = searches[str(s_id)][\"organic_results\"]\n",
    "            \n",
    "df_top_n = df_top_n.assign(Cited_by_cites_ids=cites_ids)\n",
    "df_top_n = df_top_n.assign(Cited_by_titles=cited_by_titles)\n",
    "df_top_n = df_top_n.assign(Cited_by_authors=cited_by_authors)\n",
    "df_top_n.head()[[\"Article Title\", \"Cites_id\", \"Cited_by_cites_ids\", \"Cited_by_titles\", \"Cited_by_authors\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d71b9ac-9ce8-4597-b2b5-a19d46a7250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cites ID: \n",
      "17771740241470960050\n",
      "\n",
      "Cited by cites IDs\n",
      "['559463397382443088', '6340776138936674554', '16766804411681372720', '7447715766504981253', '5331804836605365413', '13548556499559547747', '7817802389471101066', '15455167514840141912', '15744443975855027399', '8870515469873563525']\n",
      "\n",
      "Cited by titles\n",
      "['Representation learning: A review and new perspectives', 'Reservoir computing approaches to recurrent neural network training', 'Deep learning', 'Efficient estimation of word representations in vector space', 'Learning deep architectures for AI', 'Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.', 'Extracting and composing robust features with denoising autoencoders', 'Greedy layer-wise training of deep networks', 'Wide residual networks', '3D convolutional neural networks for human action recognition']\n",
      "\n",
      "Cited by authors\n",
      "[[{'name': 'Y Bengio', 'link': 'https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kukA0LcAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kukA0LcAAAAJ'}, {'name': 'A Courville', 'link': 'https://scholar.google.com/citations?user=km6CP8cAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=km6CP8cAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'km6CP8cAAAAJ'}, {'name': 'P Vincent', 'link': 'https://scholar.google.com/citations?user=WBCKQMsAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=WBCKQMsAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'WBCKQMsAAAAJ'}], [{'name': 'M Lukoševičius', 'link': 'https://scholar.google.com/citations?user=kfRwttkAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kfRwttkAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kfRwttkAAAAJ'}, {'name': 'H Jaeger', 'link': 'https://scholar.google.com/citations?user=0uztVbMAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=0uztVbMAAAAJ&engine=google_scholar_author&hl=en', 'author_id': '0uztVbMAAAAJ'}], [{'name': 'I Goodfellow', 'link': 'https://scholar.google.com/citations?user=iYN86KEAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=iYN86KEAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'iYN86KEAAAAJ'}, {'name': 'Y Bengio', 'link': 'https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kukA0LcAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kukA0LcAAAAJ'}, {'name': 'A Courville', 'link': 'https://scholar.google.com/citations?user=km6CP8cAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=km6CP8cAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'km6CP8cAAAAJ'}], [{'name': 'T Mikolov', 'link': 'https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=oBu8kMMAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'oBu8kMMAAAAJ'}, {'name': 'K Chen', 'link': 'https://scholar.google.com/citations?user=TKvd_Z4AAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=TKvd_Z4AAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'TKvd_Z4AAAAJ'}, {'name': 'G Corrado', 'link': 'https://scholar.google.com/citations?user=HBtozdUAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=HBtozdUAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'HBtozdUAAAAJ'}, {'name': 'J Dean', 'link': 'https://scholar.google.com/citations?user=NMS69lQAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=NMS69lQAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'NMS69lQAAAAJ'}], [{'name': 'Y Bengio', 'link': 'https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kukA0LcAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kukA0LcAAAAJ'}], [{'name': 'P Vincent', 'link': 'https://scholar.google.com/citations?user=WBCKQMsAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=WBCKQMsAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'WBCKQMsAAAAJ'}, {'name': 'H Larochelle', 'link': 'https://scholar.google.com/citations?user=U89FHq4AAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=U89FHq4AAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'U89FHq4AAAAJ'}, {'name': 'Y Bengio', 'link': 'https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kukA0LcAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kukA0LcAAAAJ'}], [{'name': 'P Vincent', 'link': 'https://scholar.google.com/citations?user=WBCKQMsAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=WBCKQMsAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'WBCKQMsAAAAJ'}, {'name': 'H Larochelle', 'link': 'https://scholar.google.com/citations?user=U89FHq4AAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=U89FHq4AAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'U89FHq4AAAAJ'}, {'name': 'Y Bengio', 'link': 'https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kukA0LcAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kukA0LcAAAAJ'}], [{'name': 'Y Bengio', 'link': 'https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=kukA0LcAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'kukA0LcAAAAJ'}, {'name': 'P Lamblin', 'link': 'https://scholar.google.com/citations?user=bn4xHHIAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=bn4xHHIAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'bn4xHHIAAAAJ'}], [{'name': 'S Zagoruyko', 'link': 'https://scholar.google.com/citations?user=EqJw1-4AAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=EqJw1-4AAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'EqJw1-4AAAAJ'}, {'name': 'N Komodakis', 'link': 'https://scholar.google.com/citations?user=xCPoT4EAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=xCPoT4EAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'xCPoT4EAAAAJ'}], [{'name': 'S Ji', 'link': 'https://scholar.google.com/citations?user=BZGj6sAAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=BZGj6sAAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'BZGj6sAAAAAJ'}, {'name': 'W Xu', 'link': 'https://scholar.google.com/citations?user=Gxz1fqwAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=Gxz1fqwAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'Gxz1fqwAAAAJ'}, {'name': 'M Yang', 'link': 'https://scholar.google.com/citations?user=uBHJx08AAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=uBHJx08AAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'uBHJx08AAAAJ'}, {'name': 'K Yu', 'link': 'https://scholar.google.com/citations?user=y5zkBeMAAAAJ&hl=en&oi=sra', 'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=y5zkBeMAAAAJ&engine=google_scholar_author&hl=en', 'author_id': 'y5zkBeMAAAAJ'}]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cites ID: \")\n",
    "print(df_top_n.iloc[0][\"Cites_id\"])\n",
    "print(\"\\nCited by cites IDs\")\n",
    "print(df_top_n.iloc[0][\"Cited_by_cites_ids\"])\n",
    "print(\"\\nCited by titles\")\n",
    "print(df_top_n.iloc[0][\"Cited_by_titles\"])\n",
    "print(\"\\nCited by authors\")\n",
    "print(df_top_n.iloc[0][\"Cited_by_authors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4991a26",
   "metadata": {},
   "source": [
    "# Create pairs of papers based on citation info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f8fe3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2510\n",
      "2007\n",
      "2007\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "# Remove duplicates\n",
    "df_top_n.drop_duplicates(subset=[\"Cites_id\"], inplace=True)\n",
    "# Create positive and negative pairs based on cites_id\n",
    "# [paper cited, citing paper]\n",
    "p_pairs = []\n",
    "n_pairs = []\n",
    "citing_pair_ids = []\n",
    "print(df_top_n.shape[0])\n",
    "for i in range(df_top_n.shape[0]):\n",
    "    paper_id = df_top_n.iloc[i][\"Cites_id\"]\n",
    "    citing_ids = df_top_n.iloc[i][\"Cited_by_cites_ids\"]\n",
    "    citing_papers = df_top_n[df_top_n['Cites_id'].isin(citing_ids)]\n",
    "    n_p_pairs = 0\n",
    "    for index, row in citing_papers.iterrows():\n",
    "        citing_pair_ids.append(row.Cites_id)\n",
    "        p_pairs.append([paper_id, row.Cites_id])\n",
    "        n_p_pairs += 1   \n",
    "    # sample the same number of negative examples\n",
    "    not_citing_papers = df_top_n[~df_top_n['Cites_id'].isin([paper_id, *citing_ids])]\n",
    "    n_indexes = []\n",
    "    for j in range(n_p_pairs):\n",
    "        while True:\n",
    "            random_index = randint(0, not_citing_papers.shape[0] - 1)\n",
    "            if (random_index not in n_indexes) and (random_index != i):\n",
    "                n_indexes.append(random_index)\n",
    "                break; \n",
    "    for index, row in not_citing_papers.iloc[n_indexes].iterrows():\n",
    "        n_pairs.append([paper_id, row.Cites_id])\n",
    "\n",
    "print(len(p_pairs))\n",
    "print(len(n_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17887b6c-e784-4283-9cf8-6d7bf4b96241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566     [559463397382443088, 6340776138936674554, 1676...\n",
       "4897    [15853904744099473538, 4107479906416396359, 12...\n",
       "270     [15107175476013490880, 16771076966357002396, 1...\n",
       "32      [12864264545474230024, 2690735549741753526, 11...\n",
       "3224    [14852114597417537063, 16441576239143306181, 8...\n",
       "Name: Cited_by_cites_ids, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_n.head()[\"Cited_by_cites_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50de5a90-9f08-4c68-b31f-72f58185df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add abstracts to pairs\n",
    "# [paper cited, citing paper, paper cited abstract, citing paper abstract]\n",
    "for i, pair in enumerate(p_pairs):\n",
    "    p_pairs[i].append(df_top_n.loc[df_top_n.Cites_id == pair[0], \"Abstract\"].values[0])\n",
    "    p_pairs[i].append(df_top_n.loc[df_top_n.Cites_id == pair[1], \"Abstract\"].values[0])\n",
    "    p_pairs[i].append(df_top_n.loc[df_top_n.Cites_id == pair[0], \"Authors\"].values[0])\n",
    "    p_pairs[i].append(df_top_n.loc[df_top_n.Cites_id == pair[1], \"Authors\"].values[0])\n",
    "for i, pair in enumerate(n_pairs):\n",
    "    n_pairs[i].append(df_top_n.loc[df_top_n.Cites_id == pair[0], \"Abstract\"].values[0])\n",
    "    n_pairs[i].append(df_top_n.loc[df_top_n.Cites_id == pair[1], \"Abstract\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93297ed2-66cc-4cd6-b158-d982af8a48ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['17771740241470960050', '13548556499559547747', 'Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.', 'Sensors in everyday devices, such as our phones, wearables, and computers, leave a stream of digital traces. Personal sensing refers to collecting and analyzing data from sensors embedded in the context of daily life with the aim of identifying human behaviors, thoughts, feelings, and traits. This article provides a critical review of personal sensing research related to mental health, focused principally on smartphones, but also including studies of wearables, social media, and computers. We provide a layered, hierarchical model for translating raw sensor data into markers of behaviors and states related to mental health. Also discussed are research methods as well as challenges, including privacy and problems of dimensionality. Although personal sensing is still in its infancy, it holds great promise as a method for conducting mental health research and as a clinical tool for monitoring at-risk populations and providing the foundation for the next generation of mobile health (or mHealth) interventions.', 'LeCun, Y; Bengio, Y; Hinton, G', 'Mohr, DC; Zhang, M; Schueller, SM']\n"
     ]
    }
   ],
   "source": [
    "print(p_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44ff4830-5cf9-4235-b956-482fd1a2b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['17771740241470960050', '17826593427668687115', 'Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.', 'This study presents a novel procedure based on ensemble empirical mode decomposition (EEMD) and optimized support vector machine (SVM) for multi-fault diagnosis of rolling element bearings. The vibration signal is adaptively decomposed into a number of intrinsic mode functions (IMFs) by EEMD. Two types of features, the EEMD energy entropy and singular values of the matrix whose rows are IMFs, are extracted. EEMD energy entropy is used to specify whether the bearing has faults or not. If the bearing has faults, singular values are input to multi-class SVM optimized by inter-cluster distance in the feature space (ICDSVM) to specify the fault type. The proposed method was tested on a system with an electric motor which has two rolling bearings with 8 normal working conditions and 48 fault working conditions. Five groups of experiments were done to evaluate the effectiveness of the proposed method. The results show that the proposed method outperforms other methods both mentioned in this paper and published in other literatures. (C) 2013 Elsevier Ltd. All rights reserved.']\n"
     ]
    }
   ],
   "source": [
    "print(n_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03e26996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7817802389471101066\n"
     ]
    }
   ],
   "source": [
    "print(p_pairs[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee994e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691\n"
     ]
    }
   ],
   "source": [
    "citing_ids = []\n",
    "for pair in p_pairs:\n",
    "    citing_ids.append(pair[1])\n",
    "print(len(set(citing_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77e4e1",
   "metadata": {},
   "source": [
    "## Write pairs to files for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "638cc371-8d11-4abc-a9aa-e7cfd2bdfed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open (\"Not_Cited_Pairs.csv\", \\'w\\') as pairs_file:\\n    pairs_file.write(\"paper cited\\tciting paper\\tpaper cited abstract\\tciting paper abstract\\n\")\\n    for pair in n_pairs:\\n        for i, o in enumerate(pair):\\n            pairs_file.write(str(o))\\n            if i < len(pair) - 1:\\n                pairs_file.write(\"\\t\")\\n        pairs_file.write(\"\\n\")\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using \\t as delimeter to make sure the same symbol is not found in abstracts\n",
    "with open (\"Cited_Pairs_Authors.csv\", 'w') as pairs_file:\n",
    "    pairs_file.write(\"paper cited\\tciting paper\\tpaper cited abstract\\tciting paper abstract\\tpaper cited authors\\tciting paper authors\\n\")\n",
    "    for pair in p_pairs:\n",
    "        for i, o in enumerate(pair):\n",
    "            pairs_file.write(str(o))\n",
    "            if i < len(pair) - 1:\n",
    "                pairs_file.write(\"\\t\")\n",
    "        pairs_file.write(\"\\n\")\n",
    "\n",
    "with open (\"Not_Cited_Pairs.csv\", 'w') as pairs_file:\n",
    "    pairs_file.write(\"paper cited\\tciting paper\\tpaper cited abstract\\tciting paper abstract\\n\")\n",
    "    for pair in n_pairs:\n",
    "        for i, o in enumerate(pair):\n",
    "            pairs_file.write(str(o))\n",
    "            if i < len(pair) - 1:\n",
    "                pairs_file.write(\"\\t\")\n",
    "        pairs_file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
